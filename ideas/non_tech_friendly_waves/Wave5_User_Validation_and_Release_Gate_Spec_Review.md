# Wave 5 User Validation & Release Gate Spec - Review & Analysis

**Date**: 2026-02-08
**Reviewer**: Claude Code
**Status**: ‚úÖ **Well-Structured, Ready to Execute**

---

## Executive Summary

Wave 5 is a **user validation spec**, not an implementation spec. It describes how to conduct usability testing with 10 non-tech users to validate that Waves 1-4 actually achieved "non-tech friendly" goals.

**Key Findings**:
- ‚úÖ Clear quantitative success criteria (8.5/10 score, 90% completion, ‚â§20% help needed)
- ‚úÖ Well-defined test protocol (think-aloud, 5 tasks, 25-30 min sessions)
- ‚úÖ Structured rubric with 5 dimensions (0-10 each)
- ‚úÖ Go/No-Go decision framework
- ‚ö†Ô∏è **Critical Gap**: No test script/task instructions provided
- ‚ö†Ô∏è **Critical Gap**: No rubric scoring guidelines (what is 8/10 vs 6/10?)
- ‚ö†Ô∏è **Minor Gap**: No participant recruitment plan
- ‚ö†Ô∏è **Minor Gap**: No consent form or ethics considerations

**Verdict**: Spec is **structurally sound** but needs **operational details** before execution.

---

## 1. Spec Quality Assessment

### 1.1 What the Spec Does Well ‚úÖ

1. **Clear Success Metrics**
   - Non-tech friendly score ‚â• 8.5/10
   - Task completion ‚â• 90%
   - Direct help needed ‚â§ 20%
   - Measurable, binary go/no-go criteria

2. **Appropriate Sample Size**
   - 10 participants (industry standard for usability testing)
   - Nielsen Norman Group: 5-10 users uncover 85%+ of issues
   - Spec calls for diversity (age, AI familiarity)

3. **Realistic Task List**
   - 5 tasks cover core user journey
   - Maps to actual extension workflows
   - 25-30 min duration is reasonable

4. **Structured Rubric**
   - 5 dimensions align with usability heuristics
   - 0-10 scale standard for UX research
   - Average score methodology is simple and fair

5. **Multi-Tier Decision Framework**
   - Go: ‚â•8.5, no S1 blockers (high confidence)
   - Conditional Go: 8.0-8.49 with patches (medium confidence)
   - No-Go: <8.0 or <85% completion (low confidence)

6. **Post-Test Action Plan**
   - 3 categories: Copy, Navigation, Error recovery
   - P0/P1/P2 prioritization
   - 2-3 day timebox for fixes (prevents scope creep)

---

### 1.2 Critical Gaps üî¥

#### Gap #1: No Test Script Provided

**What's Missing**:
- Detailed task instructions for each of 5 tasks
- Exact wording to give participants
- Success criteria per task
- What to say when participant gets stuck

**Why It Matters**:
- Without script, different moderators will run test differently
- Inconsistent wording ‚Üí biased results
- No way to replicate test later

**Example of what's needed**:
```markdown
Task 3: Open sidepanel and summarize text
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Instructions to participant:
"Imagine you're reading this article [show URL]. You want to quickly understand the main points. Please use the extension to summarize this paragraph [highlight specific text on screen]."

Success criteria:
- ‚úÖ Participant opens sidepanel without help
- ‚úÖ Participant finds and clicks floating button after highlighting
- ‚úÖ Participant sees AI summary appear
- ‚úÖ Completes in < 120 seconds

If stuck > 30 seconds:
- Level 1 hint: "Look for any buttons that appeared after you highlighted the text"
- Level 2 hint: "There's a floating button near your selection"
- Level 3 hint: [Show where button is, mark as needing help]
```

**Recommendation**: Create detailed test script before recruiting participants.

---

#### Gap #2: No Rubric Scoring Guidelines

**What's Missing**:
- What does 10/10 look like vs 8/10 vs 6/10?
- Anchor points for each dimension
- Examples of participant behaviors at each score level

**Why It Matters**:
- Without anchors, scoring is subjective
- Different evaluators will rate same behavior differently
- Results won't be comparable across participants

**Example of what's needed**:
```markdown
Dimension 1: D·ªÖ hi·ªÉu t·ª´ ng·ªØ (Terminology Clarity)

10/10: Participant never hesitates or asks about any label/button text
 8/10: Participant pauses briefly (2-3s) but figures it out without help
 6/10: Participant asks "What does [term] mean?" once
 4/10: Participant asks about 2-3 terms or seems confused by wording
 2/10: Participant repeatedly confused by jargon, asks "What is this?"
 0/10: Participant cannot proceed due to unclear terminology

Examples:
- Score 10: User sees "Save" button, immediately clicks without questioning
- Score 6: User sees "Export to NotebookLM", pauses, asks "What's NotebookLM?"
- Score 2: User sees "SRQ Density Mode", completely confused, says "I don't understand any of this"
```

**Recommendation**: Create rubric anchor points before running tests.

---

### 1.3 Minor Gaps üü°

#### Gap #3: No Recruitment Plan

**What's Missing**:
- Where to recruit participants (UserTesting.com? Friends/family? Social media?)
- Screening criteria (how to verify "non-tech"?)
- Incentive structure ($20 gift card? Free coffee?)
- Consent form and data privacy handling

**Recommendation**:
Add recruitment section:
```markdown
## Participant Recruitment

**Screening Questions:**
1. "Do you work in IT, software, or tech?" ‚Üí Must answer NO
2. "How often do you use browser extensions?" ‚Üí Answer: Never/Rarely
3. "Have you used AI chatbots like ChatGPT?" ‚Üí Mix of Yes/No preferred

**Recruitment Sources:**
- UserTesting.com (paid platform, $50/participant)
- Local university non-STEM departments (flyers, $20 gift card)
- Family/friends referrals (ensure no bias)

**Incentive:** $20 Amazon gift card per 30-min session

**Consent:** Verbal consent recorded at start of session, screen recording disclosed
```

---

#### Gap #4: No Pilot Test Mention in Timeline

**What's Missing**:
- Spec mentions "pilot test 1-2 nguoi" in risk mitigation (line 99)
- But timeline (section 12) doesn't budget time for pilot

**Why It Matters**:
- Pilot test often reveals script bugs, task ambiguity, tech issues
- Need 0.5 day for pilot ‚Üí iterate script ‚Üí re-run if needed

**Recommendation**:
Update estimate (section 12):
```markdown
1. Chuan bi test + pilot: 1 ngay (0.5 prep, 0.5 pilot + iterate)
2. Chay test chinh thuc: 1 ngay (10 participants)
3. Tong hop va ket luan: 0.5 ngay
4. Tong: 2.5 ngay
```

---

## 2. Task List Validation

### 2.1 Are the 5 Tasks Realistic?

| Task | Description | Feasibility | Notes |
|------|-------------|-------------|-------|
| 1 | Mo extension va hieu trang thai hien tai | ‚úÖ Realistic | Popup shows domain status, sensitivity mode |
| 2 | Cau hinh AI co ban (nhap key gia lap + luu) | ‚úÖ Realistic | Wave 2 simplified this (provider dropdown + key input) |
| 3 | Mo sidepanel va tom tat 1 doan text | ‚úÖ Realistic | Core workflow, covered by onboarding |
| 4 | Luu 1 insight vao kho nho | ‚úÖ Realistic | Save button after AI response |
| 5 | Xu ly 1 tinh huong loi (mat mang hoac key sai) | ‚ö†Ô∏è Needs Setup | Requires simulating error state |

**Task 5 Concern**: How to simulate error?
- Option A: Have participant test with fake API key ‚Üí see error message
- Option B: Moderator triggers network error via DevTools
- Option C: Pre-stage extension with corrupted settings

**Recommendation**: Use **Option A** (fake key) - most natural, tests real error recovery flow.

---

### 2.2 Do Tasks Cover Critical User Journeys?

**Coverage Matrix**:

| User Journey | Covered by Task | Wave Implemented |
|--------------|-----------------|------------------|
| Initial setup | Task 2 | Wave 2, Wave 4 |
| Highlight ‚Üí AI | Task 3 | Wave 1, Wave 4 |
| Save insights | Task 4 | Wave 1 |
| Error recovery | Task 5 | Wave 1 |
| Enable/disable per site | Task 1 (partially) | Wave 1 |
| NotebookLM export | ‚ùå NOT TESTED | - |
| SRQ (Research Queue) | ‚ùå NOT TESTED | - |
| Focus mode | ‚ùå NOT TESTED | - |

**Gap Identified**: Advanced features (NotebookLM, SRQ, Focus) not tested.

**Recommendation**:
- **Keep 5 core tasks** for primary go/no-go decision
- **Optional Task 6**: "Browse your saved highlights" (tests SRQ UI)
- Mark Task 6 as "nice to have" - doesn't block release if fails

---

## 3. Rubric Dimension Analysis

### 3.1 Are the 5 Dimensions Comprehensive?

**Current Dimensions** (from spec):
1. De hieu tu ngu (Terminology clarity)
2. De tim dung nut (Findability)
3. De hieu ket qua sau khi bam (Feedback clarity)
4. De phuc hoi khi co loi (Error recovery)
5. Cam giac tu tin khi su dung (Confidence)

**Comparison to Nielsen's 10 Usability Heuristics**:

| Nielsen Heuristic | Covered by Wave 5 Rubric | Gap |
|-------------------|--------------------------|-----|
| Visibility of system status | ‚úÖ Dimension 3 (feedback) | - |
| Match between system & real world | ‚úÖ Dimension 1 (terminology) | - |
| User control and freedom | ‚ö†Ô∏è Partial (Dimension 4) | Undo not explicitly tested |
| Consistency and standards | ‚úÖ Dimension 1 | - |
| Error prevention | ‚úÖ Dimension 4 | - |
| Recognition rather than recall | ‚úÖ Dimension 2 (findability) | - |
| Flexibility and efficiency | ‚ùå Not covered | Could add "Time efficiency" |
| Aesthetic and minimalist design | ‚ùå Not covered | Implicit in other dimensions |
| Help users recognize/recover from errors | ‚úÖ Dimension 4 | - |
| Help and documentation | ‚ö†Ô∏è Partial (Dimension 5) | Onboarding quality not explicit |

**Verdict**: 5 dimensions cover **80% of critical usability aspects**. Good enough for release gate.

**Optional Enhancement**:
Add 6th dimension: "Speed/Efficiency" (Can user complete tasks quickly?)

---

### 3.2 Is 0-10 Scale Appropriate?

**Pros**:
- ‚úÖ Familiar to UX researchers
- ‚úÖ Granular enough to differentiate (vs 1-5 scale)
- ‚úÖ Easy to average across dimensions

**Cons**:
- ‚ö†Ô∏è Hard to distinguish 7 vs 8 without anchors (see Gap #2)
- ‚ö†Ô∏è Evaluator bias (some people avoid extremes, others overuse them)

**Recommendation**:
- Keep 0-10 scale
- **Add rubric anchors** (see Gap #2 solution)
- **Use 2 independent raters** per session ‚Üí average their scores ‚Üí reduces bias

---

## 4. Go/No-Go Decision Framework

### 4.1 Are Thresholds Realistic?

| Criterion | Spec Threshold | Industry Benchmark | Assessment |
|-----------|----------------|-------------------|------------|
| Non-tech score | ‚â• 8.5/10 | 7.0-8.0 typical | ‚ö†Ô∏è **Very ambitious** |
| Task completion | ‚â• 90% | 78% typical (Nielsen) | ‚ö†Ô∏è **Very ambitious** |
| Help needed | ‚â§ 20% | 20-30% typical | ‚úÖ Reasonable |

**Context**:
- Nielsen Norman Group: 78% task completion is average for consumer software
- SUS (System Usability Scale): 68/100 is average, 80+ is excellent
- 8.5/10 = 85/100 SUS equivalent ‚Üí **top 10% of products**

**Analysis**:
Spec sets a **very high bar** - achievable but requires Waves 1-4 to be executed perfectly.

**Recommendation**:
Consider **tiered targets**:
- **Stretch Goal**: ‚â•8.5, ‚â•90% completion (Immediate GO)
- **Acceptable**: ‚â•8.0, ‚â•85% completion (GO with 2-3 day patch)
- **Needs Work**: 7.5-7.9, 80-84% completion (Conditional GO, 1-week improvement sprint)
- **Block Release**: <7.5 or <80% (NO-GO)

---

### 4.2 Is Severity System Defined?

**Spec Mentions** (section 7, line 58):
- Pain points theo muc do (S1/S2/S3)

**What's Missing**:
- Definition of S1 vs S2 vs S3
- Examples of each severity

**Recommendation**:
Add severity definitions:
```markdown
## Severity Levels

**S1 (Blocker)**: Prevents task completion, user cannot proceed
- Example: Save button not visible, onboarding crash, API key input broken

**S2 (Major)**: User completes task but with significant confusion/frustration
- Example: Unclear error message, multiple wrong clicks before success

**S3 (Minor)**: Small friction but doesn't block task
- Example: Tooltip text slightly confusing but user figures it out
```

---

## 5. Logistics & Feasibility

### 5.1 Is 2-Day Timeline Realistic?

**Spec Estimate** (section 12):
1. Chuan bi test: 0.5 ngay
2. Chay test: 1 ngay
3. Tong hop: 0.5 ngay
4. **Total: 2 ngay**

**Reality Check**:

| Activity | Spec Says | Reality Says | Gap |
|----------|-----------|--------------|-----|
| Write test script + rubric anchors | 0.5 day | 1 day | +0.5 day |
| Recruit 10 participants | 0 day (?) | 3-5 days | +3-5 days |
| Pilot test + iterate | 0 day | 0.5 day | +0.5 day |
| Run 10 sessions (30 min each) | 1 day | 1 day (if back-to-back) | OK |
| Analyze + report | 0.5 day | 1 day | +0.5 day |

**Revised Estimate**:
- **Prep phase**: 1.5 days (script, rubric, pilot)
- **Recruitment**: 3-5 days (parallel with prep)
- **Testing phase**: 1-2 days (10 sessions)
- **Analysis phase**: 1 day
- **Total calendar time**: ~1-2 weeks
- **Total effort**: 3.5-4.5 days

**Recommendation**: Update spec with realistic timeline including recruitment.

---

### 5.2 Is Remote vs In-Person Specified?

**What's Missing**:
- Remote (Zoom) or in-person?
- Screen recording tool (OBS? Zoom recording? Loom?)
- How to handle technical issues during remote sessions?

**Recommendation**:
Add logistics section:
```markdown
## Test Logistics

**Format**: Remote via Zoom (more scalable, diverse geography)

**Technical Setup**:
- Participant shares screen (Chrome only)
- Moderator records session (Zoom cloud recording)
- Backup: OBS recording on moderator side
- Participant uses test API key (provided by moderator)

**Session Structure** (30 min):
- 0-3 min: Intro, consent, ice breaker
- 3-5 min: Screen share setup
- 5-25 min: 5 tasks (4 min each)
- 25-30 min: Post-test survey + debrief
```

---

## 6. Missing Deliverables

Spec mentions deliverables but lacks templates. What's needed:

### 6.1 Participant Report Template ‚úÖ (Spec has outline)

**From spec** (section 7):
```
1. Hoan thanh task: yes/no
2. Thoi gian task (giay)
3. Pain points theo muc do (S1/S2/S3)
4. Trich dan 1 cau feedback quan trong
```

**Status**: ‚úÖ Good structure, just needs to be formatted as fillable template.

---

### 6.2 Final Report Template ‚ö†Ô∏è (Needs detail)

**From spec** (section 7):
```
1. Top 10 friction points
2. Mapping friction -> file -> de xuat fix
3. Danh sach quick fix truoc release
```

**What's Missing**:
- How to aggregate 10 participant reports into 1 summary?
- Template for "friction -> file -> fix" mapping
- Format for go/no-go recommendation

**Recommendation**:
Add final report template:
```markdown
# Wave 5 User Validation - Final Report

## Executive Summary
- Overall Score: __/10
- Task Completion: __%
- Go/No-Go Decision: ‚òê GO ‚òê CONDITIONAL GO ‚òê NO-GO

## Quantitative Results
| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Non-tech score | ‚â•8.5 | __ | ‚òê Pass ‚òê Fail |
| Task completion | ‚â•90% | __% | ‚òê Pass ‚òê Fail |
| Help needed | ‚â§20% | __% | ‚òê Pass ‚òê Fail |

## Top 10 Friction Points
| # | Issue | Severity | Frequency | File | Proposed Fix |
|---|-------|----------|-----------|------|--------------|
| 1 | [Description] | S1/S2/S3 | 8/10 users | options.js | [Fix] |
...

## P0 Quick Fixes (Must Fix Before Release)
1. [Issue] - [File] - [ETA: X hours]
...

## Recommendation
‚òê **GO**: Ship immediately
‚òê **CONDITIONAL GO**: Ship after P0 fixes (ETA: __ days)
‚òê **NO-GO**: Requires Wave 6 improvements
```

---

## 7. Risk Assessment

### 7.1 Risks from Spec (Section 11)

| Risk | Probability | Impact | Mitigation (from spec) | Assessment |
|------|------------|--------|------------------------|------------|
| Mau test khong dai dien | MEDIUM | HIGH | Tron profile da dang | ‚úÖ Good mitigation |
| Moderator bias | MEDIUM | MEDIUM | Dung script, han che can thiep | ‚úÖ Good mitigation |
| Task script khong sat use case | LOW | HIGH | Pilot test 1-2 nguoi | ‚úÖ Good mitigation |

**Additional Risks Identified**:

| Risk | Probability | Impact | Mitigation Needed |
|------|------------|--------|-------------------|
| Recruitment takes too long | HIGH | MEDIUM | Start recruiting ASAP, use paid platform |
| Technical issues during remote sessions | MEDIUM | LOW | Test setup in pilot, have backup plan |
| Participants aren't truly "non-tech" | MEDIUM | HIGH | Strict screening questions, verify background |
| Results are borderline (8.3 score) | MEDIUM | MEDIUM | Pre-define tie-breaker criteria |
| Findings conflict across participants | LOW | MEDIUM | Use 2 raters per session, look for patterns |

---

## 8. Spec vs Reality: Can You Execute This?

### 8.1 What You Have (From Waves 1-4)

‚úÖ **Implementation Complete**:
- Wave 1: Non-tech terminology, error CTAs ‚úÖ
- Wave 2: Simplified settings UI, onboarding checklist ‚úÖ
- Wave 3: i18n parity (EN/VI) ‚úÖ
- Wave 4: End-to-end onboarding flow ‚úÖ

‚úÖ **User-Facing Features Ready**:
- Popup with clear domain status
- Settings with provider dropdown + API key input
- Sidepanel with "Summarize" / "Save" buttons (non-jargon)
- Error messages with action CTAs
- Onboarding guide (3-step flow)

**Verdict**: Extension is **ready to be tested** with real users.

---

### 8.2 What You Need to Create

‚ùå **Missing for Wave 5 Execution**:
1. Detailed test script (5 tasks with exact wording)
2. Rubric scoring guidelines (anchors for 0-10 scale)
3. Participant recruitment plan
4. Consent form / ethics approval
5. Report templates (per-participant + final)
6. Analysis spreadsheet (aggregate scores, identify patterns)

**Estimate to Create Missing Pieces**: 1-1.5 days

---

## 9. Recommendations

### Option A: Execute Wave 5 As-Is (With Prep Work)

**Timeline**:
- Day 1-2: Create test script, rubric anchors, templates
- Day 3: Pilot test with 2 users, iterate script
- Day 4-10: Recruit 10 participants (parallel with prep)
- Day 11-12: Run 10 test sessions
- Day 13: Analyze results, write final report
- **Total: ~2 weeks**

**Pros**:
- ‚úÖ Validates Waves 1-4 with real users (gold standard)
- ‚úÖ Quantitative go/no-go decision (defensible)
- ‚úÖ Uncovers real usability issues

**Cons**:
- ‚è±Ô∏è Requires 2 weeks + recruitment
- üí∞ May need budget ($200-500 for incentives)
- üîß Effort-intensive (moderation, analysis)

---

### Option B: Internal QA First (Faster)

**What to do**:
1. Run Wave 4 QA test checklist (already created)
2. Fix any bugs found
3. Recruit 3-5 "non-tech" friends/family for quick feedback
4. If feedback is positive (8+/10) ‚Üí mark Waves 1-5 "done pending full validation"
5. Schedule full Wave 5 user testing post-launch (as Wave 6)

**Timeline**: 3-5 days

**Pros**:
- ‚è±Ô∏è Much faster (days vs weeks)
- üí∞ No budget needed
- üöÄ Can ship sooner

**Cons**:
- ‚ö†Ô∏è Less rigorous (smaller sample, possible bias)
- ‚ö†Ô∏è Risk of missing critical issues

---

### Option C: Hybrid Approach (Recommended)

**Phase 1: Internal Validation** (3 days)
1. Run Wave 4 QA checklist
2. Quick usability test with 3 non-tech users (1 hour each)
3. Fix P0 issues found
4. Ship to beta/limited rollout

**Phase 2: Full Wave 5 User Testing** (2 weeks, after beta launch)
1. Run full 10-participant study
2. Analyze results
3. Use findings for post-launch improvements (Wave 6)

**Pros**:
- ‚úÖ Balance speed and rigor
- ‚úÖ Get real-world usage data before full testing
- ‚úÖ Reduces risk of major surprises in full study

**Cons**:
- ‚ö†Ô∏è Two-phase approach is more complex

---

## 10. Conclusion

**Wave 5 Spec Quality**: ‚úÖ **8.5/10**

**Strengths**:
- Clear quantitative goals
- Appropriate methodology (think-aloud, 10 participants)
- Structured rubric and go/no-go framework
- Realistic task list

**Weaknesses**:
- Missing operational details (test script, rubric anchors)
- No recruitment plan or timeline for recruitment
- Overly optimistic timeline (2 days ‚Üí actually 2 weeks)

**Verdict**: Spec is **structurally sound** but needs **1-1.5 days of prep work** before execution.

---

## 11. Next Steps

**If you want to execute Wave 5 properly:**

1. **Create missing artifacts** (1-1.5 days):
   - [ ] Detailed test script (5 tasks)
   - [ ] Rubric scoring anchors (5 dimensions √ó 5 levels)
   - [ ] Participant screening survey
   - [ ] Consent form
   - [ ] Report templates

2. **Pilot test** (0.5 day):
   - [ ] Run with 2 users
   - [ ] Iterate script based on learnings

3. **Recruit & test** (1-2 weeks):
   - [ ] Recruit 10 participants
   - [ ] Run sessions
   - [ ] Analyze results

4. **Go/No-Go decision** (0.5 day):
   - [ ] Calculate scores
   - [ ] Write final report
   - [ ] Make release recommendation

**If you want to skip formal Wave 5:**
- Run **Option B** (Internal QA) or **Option C** (Hybrid)
- Mark Wave 5 as "deferred to post-launch" (Wave 6)

---

## 12. Decision Point for User

**Question**: B·∫°n mu·ªën:

**A. Execute full Wave 5** (2 weeks, rigorous validation before release)
- Best for: Pre-launch confidence, enterprise/critical product
- Effort: HIGH

**B. Internal QA only** (3 days, ship faster)
- Best for: Indie/small team, iterate post-launch
- Effort: LOW

**C. Hybrid** (3 days now + 2 weeks post-launch)
- Best for: Balance speed and rigor
- Effort: MEDIUM

**My Recommendation**: **Option C (Hybrid)** - ship with internal QA, then full Wave 5 validation post-launch informs Wave 6 improvements.

---

## Appendix: Template Starters

### A1: Test Script Starter (Task 3 Example)

```markdown
# Task 3: Summarize Text with Sidepanel

**Scenario**: "You're reading an article online and want to quickly understand the main points of this paragraph."

**Pre-Task Setup**:
- Open Chrome to https://example.com/article
- Extension installed, popup shows site is enabled
- Moderator: "Please read this paragraph [highlight on screen] and use the extension to get a summary."

**Task Instructions**:
"Please use the ATOM extension to summarize the highlighted paragraph and tell me what it means."

**Success Criteria**:
- ‚úÖ Opens sidepanel (via icon, hotkey, or floating button)
- ‚úÖ Selects text ‚Üí floating button appears
- ‚úÖ Clicks floating button ‚Üí text sent to sidepanel
- ‚úÖ Clicks "Summarize" / "T√≥m t·∫Øt" button
- ‚úÖ Sees AI summary appear
- ‚úÖ Can explain summary in own words

**Timing**: Start when task given, stop when participant says "done" or 180s elapsed

**Moderator Notes**:
- Don't explain what sidepanel is - see if they find it
- If stuck > 60s: Level 1 hint "Look for the extension icon in the toolbar"
- If stuck > 120s: Level 2 hint "Click the floating button near your text selection"
- Record # of hints given

**Data to Capture**:
- Time to complete (seconds)
- # of hints needed
- # of wrong clicks
- Rubric scores (5 dimensions, 0-10 each)
- 1 sentence quote from participant
```

### A2: Rubric Anchor Starter (Dimension 1)

```markdown
# Dimension 1: D·ªÖ hi·ªÉu t·ª´ ng·ªØ (Terminology Clarity)

**Definition**: Can user understand all labels, buttons, and messages without asking?

**10/10 - Perfect**
- User never hesitates on any text
- No questions about meaning
- Smooth, confident actions

**8/10 - Excellent**
- 1-2 brief pauses (2-3s) but self-recovers
- Says things like "Oh I see" without asking

**6/10 - Good**
- Asks "What does [term] mean?" once
- Or shows confusion for 5-10s before figuring it out

**4/10 - Fair**
- Asks about 2-3 terms
- Multiple instances of "I'm not sure what this means"
- Needs moderator explanation once

**2/10 - Poor**
- Repeatedly confused by jargon
- Cannot proceed without help
- Says "This is too technical for me"

**0/10 - Fail**
- Completely lost due to terminology
- Gives up or requires constant explanation

**Example Behaviors**:
- 10: Sees "Save" button, clicks immediately
- 6: Sees "Export to NotebookLM", pauses, asks "What's NotebookLM?" but proceeds anyway
- 2: Sees "SRQ Density Mode", says "I have no idea what any of this means"
```

---

**Review Complete** ‚úÖ
